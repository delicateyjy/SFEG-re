import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from scipy.ndimage import gaussian_filter, laplace
from einops import rearrange
from thop import profile

def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):
    return nn.Sequential(
        nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False),
        nn.BatchNorm2d(oup),
        nn.SiLU()
    )

def conv_1x1_bn(inp, oup):
    return nn.Sequential(
        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),
        nn.BatchNorm2d(oup),
        nn.SiLU()
    )

class Attention(nn.Module):
    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):
        super().__init__()
        inner_dim = dim_head *  heads
        project_out = not (heads == 1 and dim_head == dim)

        self.heads = heads
        self.scale = dim_head ** -0.5

        self.attend = nn.Softmax(dim = -1)
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)

        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        ) if project_out else nn.Identity()

    def forward(self, x):
        qkv = self.to_qkv(x).chunk(3, dim=-1)
        q, k, v = map(lambda t: rearrange(t, 'b p n (h d) -> b p h n d', h = self.heads), qkv)

        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
        attn = self.attend(dots)
        out = torch.matmul(attn, v)
        out = rearrange(out, 'b p h n d -> b p n (h d)')
        return self.to_out(out)
    
class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn
    
    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs)
    
class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim, dropout=0.):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout)
        )
    
    def forward(self, x):
        return self.net(x)
    
class Transformer(nn.Module):
    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):
        super().__init__()
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                PreNorm(dim, Attention(dim, heads, dim_head, dropout)),
                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))
            ]))
    
    def forward(self, x):
        for attn, ff in self.layers:
            x = attn(x) + x
            x = ff(x) + x
        return x
    
class MobileViTBlock(nn.Module):
    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):
        super().__init__()
        self.ph, self.pw = patch_size

        self.conv1 = conv_nxn_bn(channel, channel, kernel_size)
        self.conv2 = conv_1x1_bn(channel, dim)

        self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout)

        self.conv3 = conv_1x1_bn(dim, channel)
        self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)
    
    def forward(self, x):
        y = x.clone()

        # Local representations
        x = self.conv1(x)
        x = self.conv2(x)
        
        # Global representations
        _, _, h, w = x.shape
        x = rearrange(x, 'b d (h ph) (w pw) -> b (ph pw) (h w) d', ph=self.ph, pw=self.pw)
        x = self.transformer(x)
        x = rearrange(x, 'b (ph pw) (h w) d -> b d (h ph) (w pw)', h=h//self.ph, w=w//self.pw, ph=self.ph, pw=self.pw)

        # Fusion
        x = self.conv3(x)
        x = torch.cat((x, y), 1)
        x = self.conv4(x)
        return x

def gaussiankernel(ch_out, ch_in, kernelsize, sigma, kernelvalue):
    n = np.zeros((ch_out, ch_in, kernelsize, kernelsize))
    n[:,:,int((kernelsize-1)/2),int((kernelsize-1)/2)] = kernelvalue 
    g = gaussian_filter(n,sigma)
    gaussiankernel = torch.from_numpy(g)
    
    return gaussiankernel.float()

def laplaceiankernel(ch_out, ch_in, kernelsize, kernelvalue):
    n = np.zeros((ch_out, ch_in, kernelsize, kernelsize))
    n[:,:,int((kernelsize-1)/2),int((kernelsize-1)/2)] = kernelvalue
    l = laplace(n)
    laplacekernel = torch.from_numpy(l)
    
    return laplacekernel.float()


class SeparableConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, bias=False):
        super(SeparableConv2d, self).__init__()
        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, 
                                groups=in_channels, bias=bias, padding=1)
        self.pointwise = nn.Conv2d(in_channels, out_channels, 
                                kernel_size=1, bias=bias)

    def forward(self, x):
        out = self.depthwise(x)
        out = self.pointwise(out)
        return out
    
class SEM(nn.Module):
    def __init__(self, ch_out, reduction=None):
        super(SEM, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv = nn.Sequential(
            nn.Conv2d(ch_out, ch_out//reduction, kernel_size=1,bias=False),
            nn.ReLU(True),
            nn.Conv2d(ch_out//reduction, ch_out, kernel_size=1,bias=False),
            nn.Sigmoid()
            )

    def forward(self, x):
        y = self.avg_pool(x)
        y = self.conv(y)
        
        return x * y.expand_as(x)

class EEM(nn.Module):
    def __init__(self, ch_in, ch_out, kernel, groups, reduction):
        super(EEM, self).__init__()
        
        self.groups = groups
        self.gk = gaussiankernel(ch_in, int(ch_in/groups), kernel, kernel-2, 0.9)
        self.lk = laplaceiankernel(ch_in, int(ch_in/groups), kernel, 0.9)
        self.gk = nn.Parameter(self.gk, requires_grad=False)
        self.lk = nn.Parameter(self.lk, requires_grad=False)
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(ch_in, int(ch_out/2), kernel_size=1,padding=0,groups=2),
            nn.PReLU(num_parameters=int(ch_out/2), init=0.05),
            nn.InstanceNorm2d(int(ch_out/2))
            )
        self.conv2 = nn.Sequential(
            nn.Conv2d(ch_in, int(ch_out/2), kernel_size=1,padding=0,groups=2),
            nn.PReLU(num_parameters=int(ch_out/2), init=0.05),
            nn.InstanceNorm2d(int(ch_out/2))
            )

        self.conv3 = nn.Sequential(
                    nn.MaxPool2d(3, stride=1, padding=1),
                    nn.Conv2d(int(ch_out/2), ch_out, kernel_size=1,padding=0,groups=2),
                    nn.PReLU(num_parameters=ch_out, init=0.01),
                    nn.GroupNorm(4, ch_out)
                    )
            
        self.sem1 = SEM(ch_out, reduction=reduction)  # let's put reduction as 2
        self.sem2 = SEM(ch_out, reduction=reduction)
        self.prelu = nn.PReLU(num_parameters=ch_out, init=0.03)
      
    def forward(self, x):
        DoG = F.conv2d(x, self.gk.to('cuda'), padding='same',groups=self.groups)
        LoG = F.conv2d(DoG, self.lk.to('cuda'), padding='same',groups=self.groups)
        DoG = self.conv1(DoG-x)
        LoG = self.conv2(LoG)
        tot = self.conv3(DoG*LoG)
        
        tot1 = self.sem1(tot)
        x1 = self.sem2(x)
        
        return self.prelu(x+x1+tot+tot1)


class SubSpace(nn.Module):
    """
    Subspace class.

    ...

    Attributes
    ----------
    nin : int
        number of input feature volume.

    Methods
    -------
    __init__(nin)
        initialize method.
    forward(x)
        forward pass.

    """

    def __init__(self, nin: int) -> None:
        super(SubSpace, self).__init__()
        self.conv_dws = nn.Conv2d(
            nin, nin, kernel_size=1, stride=1, padding=0, groups=nin
        )
        self.bn_dws = nn.BatchNorm2d(nin, momentum=0.9)
        self.relu_dws = nn.ReLU(inplace=False)

        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)

        self.conv_point = nn.Conv2d(
            nin, 1, kernel_size=1, stride=1, padding=0, groups=1
        )
        self.bn_point = nn.BatchNorm2d(1, momentum=0.9)
        self.relu_point = nn.ReLU(inplace=False)

        self.softmax = nn.Softmax(dim=2)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = self.conv_dws(x)
        out = self.bn_dws(out)
        out = self.relu_dws(out)

        out = self.maxpool(out)

        out = self.conv_point(out)
        out = self.bn_point(out)
        out = self.relu_point(out)

        m, n, p, q = out.shape
        out = self.softmax(out.view(m, n, -1))
        out = out.view(m, n, p, q)

        out = out.expand(x.shape[0], x.shape[1], x.shape[2], x.shape[3])

        out = torch.mul(out, x)

        out = out + x

        return out


class ULSAM(nn.Module):
    """
    Grouped Attention Block having multiple (num_splits) Subspaces.

    ...

    Attributes
    ----------
    nin : int
        number of input feature volume.

    nout : int
        number of output feature maps

    h : int
        height of a input feature map

    w : int
        width of a input feature map

    num_splits : int
        number of subspaces

    Methods
    -------
    __init__(nin)
        initialize method.
    forward(x)
        forward pass.

    """

    def __init__(self, nin: int, nout: int, h: int, w: int, num_splits: int) -> None:
        super(ULSAM, self).__init__()

        assert nin % num_splits == 0

        self.nin = nin
        self.nout = nout
        self.h = h
        self.w = w
        self.num_splits = num_splits

        self.subspaces = nn.ModuleList(
            [SubSpace(int(self.nin / self.num_splits)) for i in range(self.num_splits)]
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        group_size = int(self.nin / self.num_splits)

        # split at batch dimension
        sub_feat = torch.chunk(x, self.num_splits, dim=1)

        out = []
        for idx, l in enumerate(self.subspaces):
            out.append(self.subspaces[idx](sub_feat[idx]))

        out = torch.cat(out, dim=1)

        return out


class EfficientCrackNet(nn.Module):
    def __init__(self, img_ch=3, output_ch=1):
        super(EfficientCrackNet, self).__init__()
        self.maxpool = nn.MaxPool2d(2,2)

        self.DSC1 = SeparableConv2d(in_channels=3, out_channels=16, kernel_size=3, bias=False)
        self.batch_norm1 = nn.BatchNorm2d(16)
        self.EEM1 = EEM(ch_in=16, ch_out=16, kernel=3, groups=1, reduction=2)

        self.DSC2 = SeparableConv2d(in_channels=16, out_channels=32, kernel_size=3, bias=False)
        self.batch_norm2 = nn.BatchNorm2d(32)
        self.EEM2 = EEM(ch_in=32, ch_out=32, kernel=3, groups=1, reduction=2)

        self.DSC3 = SeparableConv2d(in_channels=32, out_channels=64, kernel_size=3, bias=False)
        self.batch_norm3 = nn.BatchNorm2d(64)
        self.EEM3 = EEM(ch_in=64, ch_out=64, kernel=3, groups=1, reduction=2)
        self.ULSAM1 = ULSAM(64, 64, 14, 28, 4)

        self.DSC4 = SeparableConv2d(in_channels=64, out_channels=16, kernel_size=3, bias=False)
        self.batch_norm4 = nn.BatchNorm2d(16)
        self.MobileViTBlock1 = MobileViTBlock(dim=32, depth=3, channel=16, kernel_size=3, patch_size=(2,2), mlp_dim=int(32*2))

        self.DSC5 = SeparableConv2d(in_channels=16, out_channels=32, kernel_size=3, bias=False)
        self.batch_norm5 = nn.BatchNorm2d(32)
        self.MobileViTBlock2 = MobileViTBlock(dim=64, depth=3, channel=32, kernel_size=3, patch_size=(2,2), mlp_dim=int(64*2))

        self.DSC6 = SeparableConv2d(in_channels=32, out_channels=64, kernel_size=3, bias=False)
        self.batch_norm6 = nn.BatchNorm2d(64)

        self.final_conv1 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)
        self.final_ulsam = ULSAM(32, 32, 24, 34, 4)
        self.final_conv2 = nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0)

        self.upsampling1 = nn.Upsample(scale_factor=2)
        self.DSC7 = SeparableConv2d(in_channels=128, out_channels=64, kernel_size=3, bias=False)
        self.batch_norm7 = nn.BatchNorm2d(64)

        self.upsampling2 = nn.Upsample(scale_factor=2)
        self.DSC8 = SeparableConv2d(in_channels=96, out_channels=32, kernel_size=3, bias=False)
        self.batch_norm8 = nn.BatchNorm2d(32)
        self.ULSAM2 = ULSAM(32, 32, 48, 68, 4)

        # self.upsampling3 = nn.Upsample(scale_factor=2)
        self.DSC9 = SeparableConv2d(in_channels=48, out_channels=16, kernel_size=3, bias=False)
        self.batch_norm9 = nn.BatchNorm2d(16)
        self.MobileViTBlock3 = MobileViTBlock(dim=32, depth=3, channel=16, kernel_size=3, patch_size=(2,2), mlp_dim=int(32*2))

        self.upsampling4 = nn.Upsample(scale_factor=2)
        self.DSC10 = SeparableConv2d(in_channels=80, out_channels=64, kernel_size=3, bias=False)
        self.batch_norm10 = nn.BatchNorm2d(64)
        self.ULSAM3 = ULSAM(64, 64, 48, 68, 4)

        self.upsampling5 = nn.Upsample(scale_factor=2)
        self.DSC11 = SeparableConv2d(in_channels=96, out_channels=32, kernel_size=3, bias=False)
        self.batch_norm11 = nn.BatchNorm2d(32)
        self.ULSAM4 = ULSAM(32, 32, 96, 136, 4)

        self.DSC12 = SeparableConv2d(in_channels=48, out_channels=16, kernel_size=3, bias=False)
        self.batch_norm12 = nn.BatchNorm2d(16)

        self.last_block = nn.Conv2d(16, output_ch, kernel_size=1,bias=False)


    def forward(self, x):
        # Encoder Block 1
        encoder_block1_x = F.relu(self.batch_norm1(self.DSC1(x)))
        encoder_block1_out = self.EEM1(encoder_block1_x)
        print('encoder_block1_out:', encoder_block1_out.shape)
        # Encoder Block 2
        encoder_block2_x = F.relu(self.batch_norm2(self.DSC2(encoder_block1_out)))
        encoder_block2_out = self.EEM2(encoder_block2_x)
        print('encoder_block2_out:', encoder_block2_out.shape)
        # Encoder Block 3
        encoder_block3_x = self.maxpool(F.relu(self.batch_norm3(self.DSC3(encoder_block2_out))) )
        encoder_block3_out = self.ULSAM1(self.EEM3(encoder_block3_x))
        print('encoder_block3_out:', encoder_block3_out.shape)
        # Encoder Block 4
        encoder_block4_x = self.maxpool(F.relu(self.batch_norm4(self.DSC4(encoder_block3_out))) )
        encoder_block4_out = self.MobileViTBlock1(encoder_block4_x)
        print('encoder_block4_out:', encoder_block4_out.shape)
        # Encoder Block 5
        encoder_block5_x = F.relu(self.batch_norm5(self.DSC5(encoder_block4_out)))
        encoder_block5_out = self.MobileViTBlock2(encoder_block5_x)
        print('encoder_block5_out:', encoder_block5_out.shape)
        # Encoder Block 6
        encoder_block6_x = self.maxpool(F.relu(self.batch_norm6(self.DSC6(encoder_block5_out))) )
        # Output to be reversed
        encoder_output = self.final_conv2(self.final_ulsam(self.final_conv1(encoder_block6_x)))
        # Decoder Block 1
        decoder_block1_x = self.upsampling1(encoder_output)
        decoder_block1_x_ = torch.cat((encoder_block6_x, decoder_block1_x), dim=1)
        decoder_block1_out = F.relu(self.batch_norm7(self.DSC7(decoder_block1_x_)))
        print('decoder_block1_out:', decoder_block1_out.shape)
        # Decoder Block 2
        decoder_block2_x = self.upsampling2(decoder_block1_out)
        decoder_block2_x_ = torch.cat((encoder_block5_out, decoder_block2_x), dim=1)
        decoder_block2_out = self.ULSAM2(F.relu(self.batch_norm8(self.DSC8(decoder_block2_x_))))
        # Decoder Block 3
        decoder_block3_x = decoder_block2_out
        decoder_block3_x_ = torch.cat((encoder_block4_out, decoder_block3_x), dim=1)
        decoder_block3_out = self.MobileViTBlock3(F.relu(self.batch_norm9(self.DSC9(decoder_block3_x_))))
        # Decoder Block 4
        decoder_block4_x = self.upsampling4(decoder_block3_out)
        decoder_block4_x_ = torch.cat((encoder_block3_out, decoder_block4_x), dim=1)
        decoder_block4_out = self.ULSAM3(F.relu(self.batch_norm10(self.DSC10(decoder_block4_x_))))
        # Decoder Block 5
        decoder_block5_x = self.upsampling5(decoder_block4_out)
        decoder_block5_x_ = torch.cat((encoder_block2_out, decoder_block5_x), dim=1)
        decoder_block5_out = self.ULSAM4(F.relu(self.batch_norm11(self.DSC11(decoder_block5_x_))))
        # Decoder Block 6
        decoder_block6_x = decoder_block5_out
        decoder_block6_x_ = torch.cat((encoder_block1_out, decoder_block6_x), dim=1)
        decoder_block6_out = F.relu(self.batch_norm12(self.DSC12(decoder_block6_x_)))
        
        # last_out = F.relu(self.last_block(decoder_block6_out))
        # 换成sigmoid？
        last_out = torch.sigmoid(self.last_block(decoder_block6_out))
        return last_out
    
def test():
    x = torch.randn((1, 3, 512, 512)).cuda()  # batch size, channel,height,width

    model = EfficientCrackNet().cuda()
    flops, params = profile(model, (x,))
    print('flops: %.2f G, params: %.2f M' % (flops / 1e9, params / 1e6))
    preds = model(x)
    print(x.shape)
    print(preds.shape)

# flops: 2.12 G, params: 0.25 M
# torch.Size([1, 3, 256, 256])
# torch.Size([1, 1, 256, 256])

if __name__ == "__main__":
    test()
